{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# = Libraries to read in\n",
    "import os, sys\n",
    "\n",
    "# === key keras parts to create model ===\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer # use this instead of nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# put in matrix format and plot networks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# read in data\n",
    "\n",
    "dothraki_dict_source = 'dothraki_dict.csv'\n",
    "\n",
    "# will have issue with text after if to not specify string type\n",
    "os.chdir('/Users/MStamp/Documents/AIA Workshop/NLP Local Programme Setup/Code Structure/Attempts of Translation')\n",
    "\n",
    "dothraki_dict = pd.read_csv(dothraki_dict_source, dtype = 'str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1361 1361 1361\n",
      "giddy up\n",
      "hosh <eos>\n",
      "<sos> hosh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  2, 530], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# apply input and output sentences\n",
    "inputs = list(dothraki_dict['english'])\n",
    "outputs = [str(word) + ' <eos>' for word in dothraki_dict['dothraki']]\n",
    "output_sent_inputs = ['<sos> ' + str(word) for word in dothraki_dict['dothraki']]\n",
    "\n",
    "\n",
    "print(str(len(inputs)) + ' ' + str(len(outputs)) + ' ' + str(len(output_sent_inputs)))\n",
    "\n",
    "ex_val = 546\n",
    "print(inputs[ex_val] + '\\n' + outputs[ex_val] + '\\n' + output_sent_inputs[ex_val])\n",
    "\n",
    "\n",
    "# ==== Tokenization & Padding \n",
    "\n",
    "Total_inputs = len(inputs)\n",
    "\n",
    "input_tokenizer = Tokenizer(num_words = Total_inputs)\n",
    "\n",
    "# have to change code type\n",
    "inputs = [str(word) for word in inputs]\n",
    "\n",
    "input_tokenizer.fit_on_texts(inputs) # apply with inputs in English - issue as float object has no attribute\n",
    "\n",
    "input_int_seq = input_tokenizer.texts_to_sequences(inputs)\n",
    "input_int_seq[ex_val]\n",
    "\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "# - word2idx_inputs[ex_val]\n",
    "\n",
    "word2idx_inputs[inputs[ex_val].lower().split(' ')[0]]\n",
    "\n",
    "# == apply same to output\n",
    "\n",
    "Total_outputs = len(outputs)\n",
    "\n",
    "output_tokenizer = Tokenizer(num_words=Total_outputs, filters='') #\n",
    "output_tokenizer.fit_on_texts(outputs + output_sent_inputs)\n",
    "# should be here as combi\n",
    "\n",
    "\n",
    "output_int_seq = output_tokenizer.texts_to_sequences(outputs)\n",
    "output_input_int_seq = output_tokenizer.texts_to_sequences(output_sent_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "\n",
    "# === Padding need to get max lens in order to correctly pad sequences\n",
    "max_input_len = max(len(sen) for sen in input_int_seq)\n",
    "max_output_len = max(len(sen) for sen in output_int_seq) # not output - input - would expect to be longer as have <eos> at end\n",
    "# smaller in this case - as just word for word changes\n",
    "\n",
    "\n",
    "encoder_input_seq = pad_sequences(input_int_seq, maxlen = max_input_len)\n",
    "encoder_input_seq.shape\n",
    "encoder_input_seq[ex_val]\n",
    "# as only two words - see three output\n",
    "decoder_input_seq = pad_sequences(output_input_int_seq, maxlen = max_output_len, padding = 'post')\n",
    "decoder_input_seq[ex_val] # only two seen in any output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 0.18378 -0.12123 -0.11987 0.015227 -0.19121 -0.066074 -2.9876 0.80795 0.067338 -0.13184 -0.5274 0.44521 0.12982 -0.21823 -0.4508 -0.22478 -0.30766 -0.11137 -0.162 -0.21294 -0.46022 -0.086593 -0.24902 0.46729 -0.6023 -0.44972 0.43946 0.014738 0.27498 -0.078421 0.36009 0.12172 0.4298 -0.055345 0.4495 -0.74444 -0.26702 0.16431 -0.19335 0.13468 0.2887 0.23924 -0.23579 -0.28972 0.20149 0.048135 -0.18322 -0.15492 -0.19255 0.40271 0.16051 0.17721 0.32557 0.011625 -0.42572 0.34205 -0.45865 -0.2486 0.034128 0.03306 -0.057065 0.18136 -0.43638 0.0005709 -0.11935 -0.2195 0.16429 -0.18119 -0.19145 -0.081672 -0.2962 0.25803 0.073848 0.54213 -0.15405 -0.49256 0.091719 0.13329 -0.05253 -0.20518 0.34576 -1.0449 0.072779 -0.0003453 -0.16926 0.051019 -0.14753 0.23848 -0.40749 -0.58278 -0.48695 0.25863 -0.20531 -0.4775 0.40645 -0.038512 -2.403 -0.12421 0.63149 0.089419 0.08557 -0.20757 -0.1617 -0.29506 -0.13948 0.14202 -0.30138 -0.15806 0.52984 0.24229 0.075169 0.13792 0.90416 -0.23647 0.027788 0.099915 0.45422 0.60176 0.25044 0.29142 0.040712 -0.08121 -0.43786 -0.3015 -0.17991 -0.52149 0.029446 -0.23051 0.073955 0.34751 0.07806 0.19801 -0.32246 -0.13827 0.10076 0.56601 0.31925 0.09426 -0.045898 0.78329 0.19997 0.1619 0.41579 -0.31467 -0.036655 -0.11687 -0.17942 0.16246 0.42221 0.19588 -0.025058 -0.018717 -0.17965 0.35635 0.25853 0.13139 0.026784 0.017271 -0.14781 0.30598 -0.033228 0.15521 -0.50574 0.1295 0.14602 -0.35552 -0.43194 -0.1029 0.04736 -0.57903 -0.42488 0.67163 -0.11182 0.29306 -0.0033312 0.13091 -0.086655 0.22618 0.29357 -0.3088 -0.42705 0.3268 0.39254 0.17474 -0.19659 0.35665 0.38025 0.24257 -0.17021 0.097295 0.45248 -0.40589 0.27886 -0.33315 0.37076 0.16742 -0.28582 -0.051604 -0.090346 0.095385 0.26395 -0.30008 -0.63244 0.076666 0.14102 0.88613 -0.053817 0.26223 -0.016005 -0.040608 0.082136 -0.08159 -0.068912 -0.62239 -0.014757 -0.033402 0.25847 -0.28878 -0.27143 -0.23709 -0.11285 0.24828 0.14512 0.3373 -4.1005 -0.075261 0.32638 0.21444 0.37972 0.029263 0.24594 0.42935 0.68689 -0.58112 0.22939 -0.38889 0.41684 0.066217 0.47901 0.27427 0.41645 -0.35492 -0.14413 -0.010046 -0.42024 -0.19382 0.36156 -0.13364 -0.29853 0.47537 -0.26989 -0.083662 -0.0741 0.21815 -0.30678 -0.83499 -0.11287 -0.32612 0.12375 0.35341 -0.32607 0.32853 0.060266 -0.21991 0.35671 0.29546 -0.48159 -0.22347 0.31036 0.22132 -0.20994 -0.085675 -0.26173 -0.10764 -0.14802 0.17573 -0.17804 -0.21765 0.3073 -0.4459 0.03913 -0.22065 0.2214 0.32727 -0.40378 0.33021 -0.13942 -0.41003 -0.17526 0.21852 0.13615 0.10999 -0.33474 -0.046109 0.1078 -0.035657 -0.012921 -0.039038 0.18274 0.14654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# == get glove file\n",
    "# from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_source = '/Users/MStamp/Documents/AIA Workshop/NLP Local Programme Setup/Code Structure/Pre Made Translation Corpus/glove.42B.300d.txt'\n",
    "\n",
    "# open\n",
    "glove_file =  open(glove_source, encoding=\"utf8\")\n",
    "\n",
    "print_out_vals = True\n",
    "ind = 0\n",
    "# get dim output\n",
    "for line in glove_file:\n",
    "    if print_out_vals:\n",
    "        if ind % 10000000 == 0: # starts with comma\n",
    "            print(line)\n",
    "            \n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "    ind += 1\n",
    "glove_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2880e-01 -6.7593e-03 -2.4340e-01  4.9482e-02  5.3523e-01 -4.3863e-02\n",
      " -1.7705e+00 -3.4968e-02  3.1769e-01 -5.8027e-01  8.8206e-02 -3.6729e-01\n",
      " -5.2296e-02  6.7268e-01 -2.0580e-01 -2.3138e-01 -2.4324e-01 -4.3787e-02\n",
      " -1.6773e-01  9.5614e-02  6.2529e-02 -4.7919e-01 -1.9585e-01  2.8838e-01\n",
      " -4.2570e-01 -2.1769e-01  3.2729e-01  5.4468e-01  9.3767e-01 -4.7373e-01\n",
      " -2.0551e-01 -8.0361e-02 -4.8719e-01  5.2029e-02  5.6219e-01  2.5820e-01\n",
      " -1.5082e-01  4.0425e-01 -6.1659e-01 -2.7654e-01 -3.8402e-01 -3.1740e-01\n",
      " -2.2097e-01 -4.1795e-01 -1.4965e-01 -4.4497e-02  4.9798e-01 -3.7579e-02\n",
      "  7.2490e-02 -6.9846e-01 -6.3166e-01  2.8624e-01  6.7615e-02 -5.2663e-02\n",
      "  2.1006e-01  1.8085e-01  3.5130e-01 -1.3700e-01 -2.7910e-01 -2.1344e-01\n",
      "  2.4892e-01  5.4115e-02 -1.2008e-02  1.4010e-01  3.4760e-02 -6.4579e-01\n",
      "  3.2657e-01  1.9191e-01  3.1804e-01  2.8933e-01 -1.7503e-01 -2.1462e-01\n",
      " -2.0943e-01 -3.5404e-01  1.5801e-01  3.3117e-01  1.1115e-01 -3.7901e-01\n",
      "  5.8188e-01  1.6978e-01  1.6933e-01  9.9021e-01 -1.0299e-01  8.1427e-01\n",
      " -6.7585e-01 -3.4496e-02 -5.3243e-01  7.2931e-01 -3.2816e-01 -1.7433e-01\n",
      "  5.4918e-02 -7.6225e-01 -2.4561e-01  2.7541e-01  2.8828e-01 -1.2436e+00\n",
      " -3.2742e-01 -3.0831e-01 -4.4678e-01 -2.3036e-01 -3.2133e-01 -5.8245e-01\n",
      "  6.5605e-01  8.1942e-01 -8.1570e-01  4.8160e-01 -1.3835e-01  8.5136e-02\n",
      "  2.5913e-01  3.7154e-01  3.6087e-01 -4.4916e-01  1.5871e-01 -2.0325e-01\n",
      "  1.6526e-01  1.9026e-01  4.5484e-02  2.2985e-01  6.7343e-01 -1.7228e-02\n",
      "  3.3045e-01 -9.8850e-02  1.8250e-01  3.9587e-01  5.0317e-01  7.5545e-01\n",
      " -4.3906e-01 -6.3560e-01  1.5454e-01  5.0194e-02 -2.9877e-01 -2.6753e-01\n",
      "  3.1353e-01 -1.4370e-01 -6.6886e-01  3.5978e-01  4.4737e-01 -2.2255e-01\n",
      "  1.6679e-02 -2.4045e-01  5.5449e-01 -7.8809e-02  2.4600e-01  9.2955e-02\n",
      " -3.7373e-02  3.0215e-01  1.0050e-01  3.1506e-01  6.0226e-01  5.5959e-01\n",
      " -1.2215e-01  2.6837e-01 -8.2679e-01 -2.9379e-02  1.7350e-01 -6.5589e-01\n",
      " -2.7117e-01  2.2715e-01  3.8453e-01 -2.9517e-02 -3.0450e-01  4.3977e-01\n",
      " -4.8593e-01  4.2365e-01 -7.0111e-02 -4.6837e-01 -1.9011e-01 -2.3746e-02\n",
      " -4.7829e-02  5.4451e-01  4.6597e-01 -5.8662e-02  3.8888e-01  2.9006e-01\n",
      "  3.0576e-02  2.2720e-01 -5.7583e-01  1.2565e-01  1.9977e-01 -3.1947e-01\n",
      "  8.2808e-01 -3.0976e-02 -3.1661e-02 -4.8797e-01  4.6415e-01 -3.1599e-01\n",
      " -8.5216e-01 -7.0268e-01  8.3664e-02  5.6559e-01  4.5593e-01  9.8071e-03\n",
      "  4.1854e-01 -2.8402e-01  6.5484e-01  1.7777e-01  4.3339e-01 -5.4460e-01\n",
      "  1.1633e+00  2.9325e-01  7.4026e-01  1.9632e-01  2.4284e-01  1.6504e-01\n",
      "  6.4600e-01  4.1053e-01 -2.3752e-01 -3.1895e-01  7.4579e-01  6.3620e-01\n",
      " -6.1807e-01  5.5442e-02 -1.4914e-01 -6.4725e-01 -5.2697e-01 -6.4511e-01\n",
      " -1.9102e-01 -2.6380e-01 -1.6931e-01 -3.6162e-01  2.7033e-02 -2.0369e-01\n",
      "  1.3060e-01  3.2019e-01 -2.1124e+00  2.0008e-01  3.4881e-01 -1.4968e-01\n",
      "  3.3379e-01  6.9611e-01 -9.6421e-03 -5.0087e-01 -1.6512e-01 -5.6454e-01\n",
      " -4.2982e-01  1.1759e-01  4.7521e-01 -1.3207e-01 -2.0832e-02  7.6899e-02\n",
      " -7.4366e-02  4.5352e-01 -7.3842e-02  6.0398e-01  1.6719e-01  3.1527e-01\n",
      "  7.0987e-01  3.6808e-01  4.0145e-01 -1.0376e-01  1.9122e-03 -5.9801e-02\n",
      "  2.0957e-01  2.1146e-01  4.0668e-01  1.9950e-01  3.4589e-01  1.7253e-02\n",
      "  7.8303e-02  9.2941e-01 -7.2391e-01 -1.7390e-01  2.1626e-01 -8.5271e-01\n",
      "  6.7153e-01 -2.8578e-01  3.2857e-01  7.7583e-02 -2.4164e-01 -1.0281e-01\n",
      " -5.4860e-01 -3.1119e-01 -6.9961e-02 -1.9288e-01 -1.3204e-01 -3.1581e-01\n",
      " -1.5457e-01 -6.6062e-01  1.2630e-01  4.1621e-01  1.0281e+00  6.1761e-01\n",
      "  3.4091e-02  1.5702e-01 -3.6367e-01  3.3401e-02  5.1211e-01  1.2778e-01\n",
      "  4.0091e-01  3.8202e-01  1.8970e-01  5.8029e-01 -8.0392e-02  2.1924e-01\n",
      "  2.2515e-01 -1.2915e-01 -8.5990e-01 -2.4083e-01 -2.0393e-01 -5.7631e-01]\n"
     ]
    }
   ],
   "source": [
    "num_words = min(Total_inputs, len(word2idx_inputs) + 1)\n",
    "\n",
    "emb_size = 300\n",
    "\n",
    "embedding_matrix = zeros((num_words, emb_size))\n",
    "\n",
    "\n",
    "for word,index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None: # is there some instant of no words having an output\n",
    "        embedding_matrix[index] = embedding_vector # vector does not match output\n",
    "\n",
    "example_text = 'fridge'\n",
    "\n",
    "print(embeddings_dictionary[example_text])\n",
    "\n",
    "embedding_layer = Embedding(num_words, emb_size , weights=[embedding_matrix], input_length=max_input_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== develop model - additional structure featurea ====\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "decoder_targets_one_hot = zeros((\n",
    "        len(inputs),\n",
    "        max_output_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "# to fill with training output\n",
    "\n",
    "for i, d in enumerate(decoder_input_seq):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1\n",
    "        \n",
    "decoder_targets_one_hot[ex_val] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === encoder input \n",
    "\n",
    "nodes = 256 # unsure of impact to change this\n",
    "\n",
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(nodes, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]\n",
    "\n",
    "# ==== define decoder\n",
    "\n",
    "decoder_inputs_placeholder = Input(shape=(max_output_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, nodes)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(nodes, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "\n",
    "\n",
    "# ==== Combine for Prediction and Error \n",
    "\n",
    "decoder_dense = Dense(num_words_output, activation = 'softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs_placeholder, \n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 3, 300)       333000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 2, 256)       341760      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 570368      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 2, 256), (No 525312      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2, 1335)      343095      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,113,535\n",
      "Trainable params: 2,113,535\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-edc8c994f73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-febeff9c157a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpyd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "#Visualize Model\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "#create your model\n",
    "#then call the function on your model\n",
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-217919e4bc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m r = model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mencoder_input_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdecoder_targets_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "r = model.fit(\n",
    "    [encoder_input_seq, decoder_input_seq],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
